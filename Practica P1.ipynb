{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPxX9RZtWliTUBJ4Lcw+MaT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CHATBOT PARA CONSULTAS DE TECNICOS EN INSTALACIONES DE SISTEMA SONAR - PARTE 1\n","En esta primera parte realizaremos la extraccion de los datos de 3 pdfs donde contiene la información necesaria para el reentranamiento de los datos y generaremos las preguntas y respuestas que posteriormente guardaremos en un JSON."],"metadata":{"id":"DMH5CazbvonI"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1O15INLCiShG","executionInfo":{"status":"ok","timestamp":1738415121473,"user_tz":-60,"elapsed":10342,"user":{"displayName":"Manuel Abos","userId":"10111401988399218040"}},"outputId":"b38efd5d-a006-44f2-8286-6ebc2b5f94e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n","Collecting pypdf\n","  Downloading pypdf-5.2.0-py3-none-any.whl.metadata (7.2 kB)\n","Collecting langchain_experimental\n","  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n","Collecting langchain_openai\n","  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.32)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n","Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n","Collecting langchain-community<0.4.0,>=0.3.0 (from langchain_experimental)\n","  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n","Collecting langchain-core<0.4.0,>=0.3.32 (from langchain)\n","  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (24.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain) (3.0.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading pypdf-5.2.0-py3-none-any.whl (298 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_openai-0.3.3-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n","Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, dataclasses-json, langchain-core, langchain_openai, langchain-community, langchain_experimental\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.32\n","    Uninstalling langchain-core-0.3.32:\n","      Successfully uninstalled langchain-core-0.3.32\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.16 langchain-core-0.3.33 langchain_experimental-0.3.4 langchain_openai-0.3.3 marshmallow-3.26.0 mypy-extensions-1.0.0 pydantic-settings-2.7.1 pypdf-5.2.0 python-dotenv-1.0.1 tiktoken-0.8.0 typing-inspect-0.9.0\n"]}],"source":["# instalamos las librerias necesarias\n","!pip install langchain pypdf  langchain_experimental langchain_openai langchain openai"]},{"cell_type":"code","source":["# preparo los documentos en una lista con las rutas y en que pagina inicia el documento porque el resto contine informacion no relevante(Titulos e indices)\n","\n","datosPdfs = [\n","    {\n","        \"ruta\": './EtFu1051_C1.pdf',\n","        \"pagini\":3\n","    },\n","    {\n","        \"ruta\": './MaIn0362_C1.pdf',\n","        \"pagini\":6\n","    },\n","    {\n","        \"ruta\": './MaUs0240_C1.pdf',\n","        \"pagini\":4\n","    }\n","\n","]\n","\n","\n","print(datosPdfs[0]['ruta'])\n","print(type(datosPdfs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6kB5LVf0pri","executionInfo":{"status":"ok","timestamp":1738415123696,"user_tz":-60,"elapsed":545,"user":{"displayName":"Manuel Abos","userId":"10111401988399218040"}},"outputId":"e4a54aeb-27a5-4887-ec7a-8dc1fad6a738"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["./EtFu1051_C1.pdf\n","<class 'list'>\n"]}]},{"cell_type":"markdown","source":["**Vamos a separar el texto en encabezados porque los documentos vienen separados por ellos**"],"metadata":{"id":"DhA9-LiL8Sur"}},{"cell_type":"code","source":["import re\n","from langchain.document_loaders import PyPDFLoader\n","\n","# utilizamos expresiones regulares para detectar encabezados\n","expresion_regular = re.compile(r\"(?=\\n?\\d+(\\.\\d+)*\\s+[A-ZÁÉÍÓÚÑ]|CAPÍTULO\\s+\\d+)\", re.MULTILINE)\n","\n","\n","def split_por_cabeceras(text):\n","    headers = expresion_regular.split(text)\n","    return headers\n","\n","def extraer_texto_pdfs(list_pdfs):\n","\n","  # creamos una lista donde almacenaremos todos los chunks de los documentos para posteriormente generar las preguntas\n","  chunks_list_sonar = []\n","\n","  # realizamos un bucle para iterar entre los documentos\n","  for pdf in list_pdfs:\n","\n","\n","\n","    # cargamos el contenido\n","    loader = PyPDFLoader(pdf['ruta'])\n","    paginas = loader.load()\n","\n","    paginas = paginas[pdf['pagini'] - 1:]\n","\n","    # obtenemos todo el texto del PDF\n","    full_text = \"\\n\".join([page.page_content for page in paginas])\n","\n","    # separamos por encabezados\n","    secciones = split_por_cabeceras(full_text)\n","\n","\n","    # preprocesamos los datos para evitar valores nulos o secciones con poco contenido, tambien quitamos saltos de lineas innecesarios que ensucian el contenido\n","    for sec in secciones:\n","        # comprobamos que el valor nos None\n","        if sec is not None:\n","          # quitamos todas las secciones con menos de 80 caracteres\n","          if(len(sec) > 60):\n","            # reemplazamos los saltos de lineas por espacios\n","            sec = sec.replace(\"\\n\", \" \")\n","            # lo añadimos a la lista de todas las secciones\n","            chunks_list_sonar.append(sec)\n","\n","\n","  return chunks_list_sonar\n"],"metadata":{"id":"JAGhf9xW-H8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunks_list_sonar = extraer_texto_pdfs(datosPdfs)"],"metadata":{"id":"f8gOAZTG_fAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generacion de preguntas y respuestas\n","Para esta parte vamos a usar los chunks que hemos extraido de los pdfs y vamos a utilizar el modelo de chatgpt 3.5-turbo, para que genere preguntas y respuestas, despues lo guardara en un JSON para usarlo en el fine tuning"],"metadata":{"id":"UR9WXJF_NB9X"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.schema import HumanMessage\n","\n","# configuramos el modelo, indicando cual vamos a usar y su key\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"XXX\")\n","\n","# creamos una funcion que se encargara de ejecutar el modelo y mandarle el prompt base\n","def generar_qas(chunk):\n","    prompt = f\"\"\"Genera preguntas y respuestas basadas en el siguiente contenido:\\n\\n{chunk}\\n\\nFormato:\\nPregunta: ...\\nRespuesta: ...\"\"\"\n","\n","    response = llm([HumanMessage(content=prompt)])\n","    return response.content\n","\n","# ejecutamos la funcion iterando entre todos los chunks\n","qa_parejas = []\n","for doc in chunks_list_sonar:\n","    qas = generar_qas(doc)\n","    qa_parejas.append(qas)\n","\n","# guardamos en un json\n","import json\n","\n","with open(\"qa_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(qa_parejas, f, indent=4, ensure_ascii=False)\n","\n","print(\"Dataset de QA generado y guardado.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aX5NYT8MS9o","executionInfo":{"status":"ok","timestamp":1738415836285,"user_tz":-60,"elapsed":326652,"user":{"displayName":"Manuel Abos","userId":"10111401988399218040"}},"outputId":"02fd7a64-4fa5-4ccf-d961-bf19813103b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset de QA generado y guardado.\n"]}]},{"cell_type":"code","source":["# observamos que el dataset esta mal estructurado lo mejoramos\n","\n","\n","file_path = \"./qa_dataset.json\"\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    data = json.load(file)\n","\n","# Procesar el contenido del JSON para separar cada pregunta y respuesta en una fila\n","qa_list = []\n","for item in data:\n","    qa_pairs = item.split(\"\\n\\n\")\n","    for pair in qa_pairs:\n","        if \"Pregunta:\" in pair and \"Respuesta:\" in pair:\n","            question, answer = pair.split(\"\\nRespuesta: \", 1)\n","            question = question.replace(\"Pregunta: \", \"\").strip()\n","            answer = answer.strip()\n","            qa_list.append({\"pregunta\": question, \"respuesta\": answer})\n","\n","# guardamos en un json\n","import json\n","\n","with open(\"qa_dataset_mejorado.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(qa_list, f, indent=4, ensure_ascii=False)"],"metadata":{"id":"5abltGmv7FHV"},"execution_count":null,"outputs":[]}]}